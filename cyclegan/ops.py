# GAN OPS
# Loss,Activation,Normalization,Layer

# Activation
def lrelu():