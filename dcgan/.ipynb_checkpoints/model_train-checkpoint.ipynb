{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(218, 178, 3)\n",
      "(64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jan 31 15:11:49 2019\n",
    "\n",
    "@author: JM\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "img = cv2.imread('C:/Users/JM/Git_store/celebA/000005.jpg', cv2.IMREAD_COLOR)\n",
    "print(img.shape)\n",
    "\n",
    "res = cv2.resize(img,(64,64), interpolation = cv2.INTER_AREA)\n",
    "print(res.shape)\n",
    "\n",
    "cv2.imshow('image',res)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch normalization node\n",
    "class batch_norm(object):\n",
    "    def __init__(self, epsilon=1e-5, momentum=0.9, name=\"batch_norm\"):\n",
    "        with tf.variable_scope(name):\n",
    "            self.epsilon = epsilon\n",
    "            self.momentum = momentum\n",
    "            self.name = name\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        return tf.contrib.layers.batch_norm(x,\n",
    "                                            decay=self.momentum,\n",
    "                                            updates_collections=None,\n",
    "                                            epsilon=self.epsilon,\n",
    "                                            scale=True,\n",
    "                                            is_training=train,\n",
    "                                            scope=self.name,\n",
    "                                            reuse=tf.AUTO_REUSE  # if tensorflow vesrion < 1.4, delete this line\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "img_shape = [64,64,3]\n",
    "batch_size = 128\n",
    "total_epoch = 10\n",
    "total_batch = 100\n",
    "learning_rate = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4x4 filter\n",
    "G_W1 = tf.Variable(tf.truncated_normal([4, 4, 1024, 100], stddev=0.02), name=\"G_W1\")\n",
    "#batch-norm\n",
    "G_bn1 = batch_norm(name=\"G_bn1\")\n",
    "\n",
    "#4x4 filter\n",
    "G_W2 = tf.Variable(tf.truncated_normal([4, 4, 512, 1024], stddev=0.02), name='G_W2')\n",
    "#batch-norm\n",
    "G_bn2 = batch_norm(name=\"G_bn2\")\n",
    "\n",
    "#4x4 filter\n",
    "G_W3 = tf.Variable(tf.truncated_normal([4, 4, 256, 512], stddev=0.02), name='G_W3')\n",
    "#batch-norm\n",
    "G_bn3 = batch_norm(name=\"G_bn3\")\n",
    "\n",
    "#4x4 filter\n",
    "G_W4 = tf.Variable(tf.truncated_normal([4, 4, 128, 256], stddev=0.02), name='G_W4')\n",
    "#batch-norm\n",
    "G_bn4 = batch_norm(name=\"G_bn4\")\n",
    "\n",
    "#4x4 filter\n",
    "G_W5 = tf.Variable(tf.truncated_normal([4, 4, 3, 128], stddev=0.02), name='G_W5')\n",
    "\n",
    "\n",
    " #4x4 filter\n",
    "D_W1 = tf.Variable(tf.truncated_normal([4, 4, 3, 128], stddev=0.02), name='D_W1')\n",
    "\n",
    " #4x4 filter\n",
    "D_W2 = tf.Variable(tf.truncated_normal([4, 4, 128, 256], stddev=0.02), name='D_W2')\n",
    "#batch-norm\n",
    "D_bn2 = batch_norm(name=\"D_bn2\")\n",
    "\n",
    "#4x4 filter\n",
    "D_W3 = tf.Variable(tf.truncated_normal([4, 4, 256, 512], stddev=0.02), name='D_W3')\n",
    "#batch_norm\n",
    "D_bn3 = batch_norm(name=\"D_bn3\")\n",
    "\n",
    "#4x4 filter\n",
    "D_W4 = tf.Variable(tf.truncated_normal([4, 4, 512, 1024], stddev=0.02), name='D_W4')\n",
    "#batch_norm\n",
    "D_bn4 = batch_norm(name=\"D_bn4\")\n",
    "\n",
    "#4x4 filter\n",
    "D_W5 = tf.Variable(tf.truncated_normal([4, 4, 1024, 1], stddev=0.02), name='D_W5')\n",
    "\n",
    "D_var_list = [D_W1, D_W2, D_W3, D_W4, D_W5]\n",
    "G_var_list = [G_W1, G_W2, G_W3, G_W4, G_W5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(z):\n",
    "    #generate\n",
    "\n",
    "    #1x1x100\n",
    "    input_ = tf.reshape(z,[batch_size,1,1,100]) \n",
    "\n",
    "    #4x4 filter\n",
    "    #G_W1 = tf.Variable(tf.truncated_normal([4, 4, 1024, 100], stddev=0.02), name=\"G_W1\")\n",
    "    #batch-norm\n",
    "    #G_bn1 = batch_norm(name=\"G_bn1\")\n",
    "\n",
    "    #1x1x100 -> 4x4x1024\n",
    "    layer_1 = tf.nn.conv2d_transpose(input_ \n",
    "                                     ,G_W1\n",
    "                                     ,output_shape=[batch_size,4,4,1024]\n",
    "                                     ,strides=[1,4,4,1])\n",
    "\n",
    "    #activation function\n",
    "    layer_1 = tf.nn.relu(G_bn1(layer_1))\n",
    "\n",
    "    #4x4 filter\n",
    "    #G_W2 = tf.Variable(tf.truncated_normal([4, 4, 512, 1024], stddev=0.02), name='G_W2')\n",
    "    #batch-norm\n",
    "    #G_bn2 = batch_norm(name=\"G_bn2\")\n",
    "\n",
    "    #4x4x1024 -> 8x8x512\n",
    "    layer_2 = tf.nn.conv2d_transpose(layer_1\n",
    "                                    ,G_W2\n",
    "                                    ,output_shape=[batch_size,8,8,512]\n",
    "                                    ,strides=[1,2,2,1])\n",
    "\n",
    "    #activation function\n",
    "    layer_2 = tf.nn.relu(G_bn2(layer_2))\n",
    "\n",
    "    #4x4 filter\n",
    "    #G_W3 = tf.Variable(tf.truncated_normal([4, 4, 256, 512], stddev=0.02), name='G_W3')\n",
    "    #batch-norm\n",
    "    #G_bn3 = batch_norm(name=\"G_bn3\")\n",
    "\n",
    "    #8x8x512 -> 16x16x256\n",
    "    layer_3 = tf.nn.conv2d_transpose(layer_2\n",
    "                                    ,G_W3\n",
    "                                    ,output_shape=[batch_size,16,16,256]\n",
    "                                    ,strides=[1,2,2,1])\n",
    "\n",
    "    #activation function\n",
    "    layer_3 = tf.nn.relu(G_bn3(layer_3))\n",
    "\n",
    "    #4x4 filter\n",
    "    #G_W4 = tf.Variable(tf.truncated_normal([4, 4, 128, 256], stddev=0.02), name='G_W4')\n",
    "    #batch-norm\n",
    "    #G_bn4 = batch_norm(name=\"G_bn4\")\n",
    "\n",
    "    #16x16x256 -> 32x32x128\n",
    "    layer_4 = tf.nn.conv2d_transpose(layer_3\n",
    "                                    ,G_W4\n",
    "                                    ,output_shape=[batch_size,32,32,128]\n",
    "                                    ,strides=[1,2,2,1])\n",
    "\n",
    "    #activation function\n",
    "    layer_4 = tf.nn.relu(G_bn4(layer_4))\n",
    "\n",
    "    #4x4 filter\n",
    "    #G_W5 = tf.Variable(tf.truncated_normal([4, 4, 3, 128], stddev=0.02), name='G_W5')\n",
    "\n",
    "    #32x32x128 -> 64x64x3\n",
    "    layer_5 = tf.nn.conv2d_transpose(layer_4\n",
    "                                    ,G_W5\n",
    "                                    ,output_shape=[batch_size,64,64,3]\n",
    "                                    ,strides=[1,2,2,1])\n",
    "\n",
    "    output_ = tf.nn.tanh(layer_5)\n",
    "\n",
    "    return output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_visualization(X, nh_nw, save_path='./vis/sample.jpg'):\n",
    "    nh, nw = nh_nw\n",
    "    h, w = X.shape[1], X.shape[2]\n",
    "    img = np.zeros((h * nh, w * nw, 3))\n",
    "\n",
    "    for n, x in enumerate(X):\n",
    "        j = int(n / nw)\n",
    "        i = int(n % nw)\n",
    "        img[j * h:j * h + h, i * w:i * w + w, :] = x\n",
    "\n",
    "    cv2.imwrite(save_path, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminate(img):\n",
    "    #4x4 filter\n",
    "    #D_W1 = tf.Variable(tf.truncated_normal([4, 4, 3, 128], stddev=0.02), name='D_W1')\n",
    "\n",
    "    #64x64x3 -> 32x32x128\n",
    "    layer_1 = tf.nn.conv2d(img\n",
    "                           ,D_W1\n",
    "                           ,strides=[1,2,2,1]\n",
    "                           ,padding='SAME')\n",
    "    \n",
    "    #activation function\n",
    "    layer_1 = tf.nn.leaky_relu(layer_1,alpha=0.2)\n",
    "\n",
    "    #4x4 filter\n",
    "    #D_W2 = tf.Variable(tf.truncated_normal([4, 4, 128, 256], stddev=0.02), name='D_W2')\n",
    "    #batch-norm\n",
    "    #D_bn2 = batch_norm(name=\"D_bn2\")\n",
    "\n",
    "    #32x32x128 -> 16x16x256\n",
    "    layer_2 = tf.nn.conv2d(layer_1\n",
    "                           ,D_W2\n",
    "                           ,strides=[1,2,2,1]\n",
    "                           ,padding='SAME')\n",
    "    \n",
    "    #activation function\n",
    "    layer_2 = tf.nn.leaky_relu(D_bn2(layer_2),alpha=0.2)\n",
    "\n",
    "    #4x4 filter\n",
    "    #D_W3 = tf.Variable(tf.truncated_normal([4, 4, 256, 512], stddev=0.02), name='D_W3')\n",
    "    #batch_norm\n",
    "    #D_bn3 = batch_norm(name=\"D_bn3\")\n",
    "\n",
    "    #16x16x256 -> 8x8x512\n",
    "    layer_3 = tf.nn.conv2d(layer_2\n",
    "                           ,D_W3\n",
    "                           ,strides=[1,2,2,1]\n",
    "                           ,padding='SAME')\n",
    "    \n",
    "    #activation function\n",
    "    layer_3 = tf.nn.leaky_relu(D_bn3(layer_3),alpha=0.2)\n",
    "\n",
    "    #4x4 filter\n",
    "    #D_W4 = tf.Variable(tf.truncated_normal([4, 4, 512, 1024], stddev=0.02), name='D_W4')\n",
    "    #batch_norm\n",
    "    #D_bn4 = batch_norm(name=\"D_bn4\")\n",
    "\n",
    "    #8x8x512 -> 4x4x1024\n",
    "    layer_4 = tf.nn.conv2d(layer_3\n",
    "                           ,D_W4\n",
    "                           ,strides=[1,2,2,1]\n",
    "                           ,padding='SAME')\n",
    "    \n",
    "    #activation function\n",
    "    layer_4 = tf.nn.leaky_relu(D_bn4(layer_4),alpha=0.2)\n",
    "\n",
    "    #4x4 filter\n",
    "    #D_W5 = tf.Variable(tf.truncated_normal([4, 4, 1024, 1], stddev=0.02), name='D_W5')\n",
    "    \n",
    "    #4x4x1024 -> 1x1x1\n",
    "    layer_5 = tf.nn.conv2d(layer_4,D_W5,strides=[1,4,4,1],padding='SAME')\n",
    "    layer_5 = tf.reshape(layer_5,[batch_size,1])\n",
    "    \n",
    "    #percentage\n",
    "    output_ = tf.nn.sigmoid(layer_5)\n",
    "    \n",
    "    return output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.placeholder(tf.float32,[batch_size,100])\n",
    "img_real = tf.placeholder(tf.float32,[batch_size]+img_shape)\n",
    "\n",
    "img_fake = generate(noise)\n",
    "\n",
    "#image real,fake inspection\n",
    "d_real = discriminate(img_real)\n",
    "d_fake = discriminate(img_fake)\n",
    "\n",
    "#d_cost want d_real to get bigger\n",
    "#g_cost want d_fake to get smaller\n",
    "d_cost = tf.reduce_mean(tf.log(d_real) + tf.log(1 - d_fake))\n",
    "#g_cost want d_fake to get bigger\n",
    "g_cost = tf.reduce_mean(tf.log(d_fake))\n",
    "\n",
    "#train\n",
    "d_train = tf.train.AdamOptimizer(learning_rate=learning_rate ,beta1=0.5).minimize(-d_cost ,var_list = D_var_list)\n",
    "g_train = tf.train.AdamOptimizer(learning_rate=learning_rate ,beta1=0.5).minimize(-g_cost ,var_list = G_var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_test = np.random.normal(size=(256,100))\n",
    "\n",
    "cv2.imshow('noise',noise_test)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_noise = 100\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(total_epoch):\n",
    "        for step in range(total_batch):\n",
    "            #next batch 작성\n",
    "            batch_x = next_batch()\n",
    "            \n",
    "            #batch_x = cv2.resize(batch_x,(64,64), interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "            z = np.random.normal(size=(batch_size, n_noise))\n",
    "            \n",
    "            _, loss_val_D = sess.run([d_train, d_cost],\n",
    "                                     feed_dict={img_real: batch_x, noise: z})\n",
    "            \n",
    "            _, loss_val_G = sess.run([g_train, g_cost],\n",
    "                                     feed_dict={noise: z})\n",
    "            \n",
    "            print('Epoch: [', epoch, '/', total_epoch, '], ', 'Step: [', step, '/', total_batch, '],\n",
    "                  D_loss: ',loss_val_D, ', G_loss: ', loss_val_G)\n",
    "            \n",
    "     #test code 작성"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
